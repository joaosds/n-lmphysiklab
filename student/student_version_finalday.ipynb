{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (10000, 75)\n",
      "(10000, 75) 96\n",
      "X shape: (750000, 2), X_train: (675000, 2), X_val: (75000, 2)\n",
      "Y shape: (750000,), Y_train: (675000,), X_val: (75000,)\n",
      "Trainable parameters: 12867\n",
      "aaa\n",
      "torch.Size([200, 2]) torch.Size([200])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 147\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    146\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m--> 147\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits, loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits, loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Documents/oldlinux/phd/projects/LMPhysikLab/lmphysiklab/lib/python3.10/site-packages/torch/nn/modules/loss.py:1379\u001b[0m, in \u001b[0;36mCrossEntropyLoss.__init__\u001b[0;34m(self, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1372\u001b[0m     weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     label_smoothing: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1378\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index \u001b[38;5;241m=\u001b[39m ignore_index\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing \u001b[38;5;241m=\u001b[39m label_smoothing\n",
      "File \u001b[0;32m~/Documents/oldlinux/phd/projects/LMPhysikLab/lmphysiklab/lib/python3.10/site-packages/torch/nn/modules/loss.py:57\u001b[0m, in \u001b[0;36m_WeightedLoss.__init__\u001b[0;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     52\u001b[0m     weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight: Optional[Tensor]\n",
      "File \u001b[0;32m~/Documents/oldlinux/phd/projects/LMPhysikLab/lmphysiklab/lib/python3.10/site-packages/torch/nn/modules/loss.py:44\u001b[0m, in \u001b[0;36m_Loss.__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy_get_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n",
      "File \u001b[0;32m~/Documents/oldlinux/phd/projects/LMPhysikLab/lmphysiklab/lib/python3.10/site-packages/torch/nn/_reduction.py:44\u001b[0m, in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mand\u001b[39;00m reduce:\n\u001b[1;32m     45\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"./CSV/general_langevin_discrete_10000_75_training.csv\").to_numpy()\n",
    "data = pd.read_csv(\"./CSV/GLEtt2_10000_100_training.csv\").to_numpy()\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "# Define parameters\n",
    "number_of_bins = int(np.max(data)) + 1\n",
    "total_number_steps = data.shape[1]\n",
    "previous_steps = 2\n",
    "\n",
    "# Prepare features and labels\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(-previous_steps, total_number_steps - previous_steps):\n",
    "    if i < 0:\n",
    "        X.append(number_of_bins * np.ones((data.shape[0], previous_steps), dtype=np.int64))\n",
    "        Y.append(data[:, i + previous_steps])\n",
    "    else:\n",
    "        X.append(data[:, i:i + previous_steps])\n",
    "        Y.append(data[:, i + previous_steps])\n",
    "\n",
    "X = np.array(X).transpose(1, 0, 2).reshape(-1, previous_steps)\n",
    "Y = np.array(Y).T.reshape(-1)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(data.shape, number_of_bins)\n",
    "print(f\"X shape: {X.shape}, X_train: {X_train.shape}, X_val: {X_val.shape}\")\n",
    "print(f\"Y shape: {Y.shape}, Y_train: {Y_train.shape}, X_val: {Y_val.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).long()\n",
    "Y_train = torch.from_numpy(Y_train).long()\n",
    "X_val = torch.from_numpy(X_val).long()\n",
    "Y_val = torch.from_numpy(Y_val).long()\n",
    "\n",
    "class NGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_neurons, context_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(context_size * embed_dim, hidden_neurons)\n",
    "        self.fc2 = nn.Linear(hidden_neurons, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, context_size)\n",
    "        embeds = self.embedding(x)  # (batch_size, context_size, embed_dim)\n",
    "        embeds = embeds.view(embeds.size(0), -1)  # (batch_size, context_size * embed_dim)\n",
    "        hidden = torch.tanh(self.fc1(embeds))\n",
    "        logits = self.fc2(hidden)\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 10\n",
    "hidden_neurons = [30,30,100]\n",
    "batch_size = 200\n",
    "class NGramModel(nn.Module):\n",
    "    \"\"\"Deep feedforward neural network for n-gram modeling\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_layers, context_size, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            embed_dim: Dimension of embeddings\n",
    "            hidden_layers: List of hidden layer sizes, e.g. [200, 100] for 2 hidden layers\n",
    "            context_size: Number of previous tokens to consider\n",
    "            dropout: Dropout probability (0 = no dropout)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Build deep network\n",
    "        layers = []\n",
    "        input_size = context_size * embed_dim\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_size, vocab_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, context_size)\n",
    "        embeds = self.embedding(x)  # (batch_size, context_size, embed_dim)\n",
    "        embeds = embeds.view(embeds.size(0), -1)  # (batch_size, context_size * embed_dim)\n",
    "        logits = self.network(embeds)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "model = NGramModel(\n",
    "     vocab_size=number_of_bins + 1,\n",
    "     embed_dim=embed_dim,\n",
    "     hidden_layers=[100],  # 3 hidden layers with decreasing size\n",
    "     context_size=previous_steps,\n",
    "     dropout=0  # Add dropout for regularization\n",
    " )\n",
    "\n",
    "#model = model.double()  # Use float64 to match TensorFlow\n",
    "\n",
    "# Initialize with normal distribution (mean=0, std=1)\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param.normal_(mean=0.0, std=1.0)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {total_params}\")\n",
    "\n",
    "# Define loss and optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "epochs_list = [1]\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for lr, num_epochs in zip(learning_rates, epochs_list):\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        batch_idx = torch.randint(0, X_train.size(0), (batch_size,))\n",
    "        X_batch = X_train[batch_idx]\n",
    "        Y_batch = Y_train[batch_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_batch)\n",
    "        # TODO: introduce the loss function comparing with Y_batch\n",
    "        loss =\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation loss (compute every 10 epochs to save time)\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits_val = model(X_val)\n",
    "                loss_val = criterion(logits_val, Y_val)\n",
    "                val_losses.append(loss_val.item())\n",
    "\n",
    "# Plot learning curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Training loss\n",
    "val_epochs = [i * 10 for i in range(len(val_losses))]\n",
    "ax1.plot(range(len(losses)), np.log(losses))\n",
    "ax1.plot(val_epochs, np.log(val_losses), color='orange')\n",
    "ax1.set_title(\"Log-Loss for minibatches (Training)\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Log-Loss\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Validation loss\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_validation_loss.png\")\n",
    "print(\"Training and validation loss plots saved\")\n",
    "\n",
    "# Evaluate validation loss\n",
    "with torch.no_grad():\n",
    "    logits_val = model(X_val)\n",
    "    loss_val = criterion(logits_val, Y_val)\n",
    "    print(f\"Validation data loss: {loss_val.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmphysiklab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
